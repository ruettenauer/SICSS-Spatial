---
title: "Part 2: Analysing spatial data"
author: "Tobias RÃ¼ttenauer"
date: "June 19, 2021"
output_dir: "./docs"
output: 
  html_document:
    theme: flatly
    highlight: haddock
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 2
theme: united
bibliography: sicss-spatial.bib
link-citations: yes
---

\newcommand{\Exp}{\mathrm{E}}
\newcommand\given[1][]{\:#1\vert\:}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}

### Required packages

```{r, message = FALSE, warning = FALSE, results = 'hide'}
pkgs <- c("sf", "mapview", "spdep", "spatialreg", "tmap", "GWmodel", "viridisLite") # note: load spdep first, then spatialreg
lapply(pkgs, require, character.only = TRUE)

```

### Session info

```{r}
sessionInfo()

```

### Load spatial data

See previous file.

```{r}
load("_data/msoa_spatial.RData")

```

# Spatial interdependence

We can not only use coordinates and geo-spatial information to connect different data sources, we can also explicitly model spatial (inter)depence in the analysis of our data. In many instance, accounting for spatial dependence might even be necessary to avoid biased point estimates and standard errors. The reason is our observations are often not independent and identically distributed: 'everything is related to everything else, but near things are more related than distant things' [@Tobler.1970.0].

However, even if we would receive unbiased estimates with conventional methods, using the spatial information inherent in the data can help us detect specific patterns and spatial relations. 

## ${\bm W}$: Connectivity between units

To analyse spatial relations, we first need to define some sort of connectivity between units (e.g. similar to network analysis). There is an ongoing debate about the importance of spatial weights for spatial econometrics and about the right way to specify weights matrices [@LeSage.2014.0b, @Neumayer.2016.0]. The following graph shows some possible options in how to define connectivity between units.

![Figure: Spatial data linkage, Source: @Bivand.2018.748](fig/Bivand_neighbours.png)

In spatial econometrics, the spatial connectivity (as shown above) is usually represented by a spatial weights matrix ${\bm W}$:
$$
\begin{equation} 
		\bm W = \begin{bmatrix} 
    		w_{11} & w_{12} & \dots & w_{1n} \\
    		w_{21} & w_{22} & \dots & w_{2n} \\
    		\vdots & \vdots & \ddots & \vdots \\
    		w_{n1} & w_{n2} & \dots     & w_{nn} 
    		\end{bmatrix}
		\end{equation}
$$
Note: The diagonal elements $w_{i,i}= w_{1,1}, w_{2,2}, \dots, w_{n,n}$  of $\bm W$	are always zero. No unit is a neighbour of itself.

### Contiguity weights

A very common type of spatial weights. Binary specification, taking the value 1 for neighbouring units (queens: sharing a common edge; rook: sharing a common border), and 0 otherwise.

Contiguity weights $w_{i,j} =$

* 1 if $i$ and $j$ neighbours

* 0 \text{otherwise}

$$
		\begin{equation} 
		\bm W  = \begin{bmatrix} 
    		0 & 0 & 1  \\
    		0 & 0 & 0  \\
    		1 & 0 & 0  
    		\end{bmatrix} 	\nonumber
		\end{equation}
$$

* Sparse matrices

* Problem of `island' (units without neighbours)

Lets create a contiguity weights matrix (Queens neighbours) for the London MSOAs. Therefore, we create a neighbours list (`nb`), which is an efficient way of storing ${\bm W}$.

```{r}
# Contiguity (Queens) neighbours weights
queens.nb <- poly2nb(msoa.spdf, 
                     queen = TRUE, 
                     snap = 1) # we consider points in 1m distance as 'touching'
summary(queens.nb)

# Lets plot that
plot(st_geometry(msoa.spdf), border = "grey60")
plot(queens.nb, st_centroid(st_geometry(msoa.spdf)), 
     add = TRUE, pch = 19, cex = 0.6)

# We can also transform this into the matrix
W <- nb2mat(queens.nb, style = "B")
print(W[1:10, 1:10])

```


### Distance based weights

Another common type uses the distance $d_{ij}$ between each unit $i$ and $j$.

* Inverse distance weights $w_{i,j} =  \frac{1}{d_{ij}}$

$$
		\begin{equation} 
		\bm W = \begin{bmatrix} 
    		0 & \frac{1}{d_{ij}} & \frac{1}{d_{ij}}  \\
    		\frac{1}{d_{ij}} & 0 & \frac{1}{d_{ij}}  \\
    		\frac{1}{d_{ij}} & \frac{1}{d_{ij}} & 0  
    		\end{bmatrix} 	\nonumber
		\end{equation}
$$		

* Dense matrices

* Specifying thresholds may be useful (to get rid of very small non-zero weights)

For now, we will just specify a neighbours list with a distance threshold of 3km using `dnearneigh()`. An alternative would be k nearest neighbours using `knearneigh()`. We will do the inverse weighting later.

```{r}
# Crease centroids
coords <- st_geometry(st_centroid(msoa.spdf))

# Neighbours within 5km distance
dist_3.nb <- dnearneigh(coords, d1 = 0, d2 = 3000)
summary(dist_3.nb)

# Lets plot that
plot(st_geometry(msoa.spdf), border = "grey60")
plot(dist_3.nb, coords, 
     add = TRUE, pch = 19, cex = 0.6)

```

## Normalization of ${\bm W}$

Normalizing ensures that the parameter space of the spatial multiplier is restricted to $-1 < \rho > 1$, and the multiplier matrix is non-singular. Again, how to normalize a weights matrix is subject of debate [@LeSage.2014.0b; @Neumayer.2016.0].
	
Normalizing your weights matrix is always a good idea. Otherwise, the spatial parameters might blow up -- if you can estimate the model at all.

### Row-normalization

Row-normalization divides each non-zero weight by the sum of all weights of unit $i$, which is the sum of the row. 

$$
\frac{w_{ij}}{\sum_j^n w_{ij}}
$$ 
		
* Spatial lags are average values of neighbours

* Proportions between units (distance based) get lost

* Can induce asymmetries: $w_{ij} \neq w_{ji}$ 

For instance, we can use row-normalization for the Queens neighbours created above, and create a neighbours list with spatial weights

```{r}
queens.lw <- nb2listw(queens.nb,
                      style = "W") # W ist row-normalization
summary(queens.lw)
```

	
### Maximum eigenvalues normalization

Maximum eigenvalues normalization: Divide each non-zero weight by overall maximum eigenvalue $\lambda_{max}$. Each element of $\bm W$ is divided by the same scalar parameter. 
		 
$$
\frac{\bm W}{\lambda_{max}}
$$

* Interpretation may become more complicated

* Keeps proportions of connectivity strengths across units (relevant esp. for distance based $\bm W$)

For instance, we can use eigenvalue normalization for the inverse distance neighbours. We use `nb2listwdist()` to create weight by inverse distance and normalize in one step.

```{r}
idw.lw <- nb2listwdist(dist_3.nb,
                       x = coords, # needed for idw
                       type = "idw", # inverse distance weighting
                       alpha = 1, # the decay parameter for distance weighting
                       style = "minmax") # for eigenvalue normalization
summary(idw.lw)

```

### Islands / missings

In practice, we often have a problem with islands. If we use contiguity based or distance based nieghbours definitions, some units may end up with empty neighbours sets: they just do not touch any other unit or have a neighbour within a specific ditance. This hower creates a problem: what is the value in the neighbouring units?

The `zero.policy` option in spdep allows us to proceed with empty neihbours sets. However, many further functions may run into problems with empty neighbours sets. So it often makes sense to either drop islands, to choose weights which always have neighbours (e.g. k nearest), or impute empty neighbours sets by using the nearest neighbours.

# Spatial Autocorrelation

If spatially close observations are more likely to exhibit similar values, we cannot handle observations as if they were independent.

$$ 
\Exp(\varepsilon_i\varepsilon_j)\neq \Exp(\varepsilon_i)\Exp(\varepsilon_j) = 0
$$
		
This violates a basic assumption of the conventional OLS model. In consequence, ignoring spatial dependence can lead to
		
* biased inferential statistics

* biased point estimates (depending on the DGP)

### Visualization

There is one very easy and intuitive way of detecting spatial autocorrelation: Just look at the map. We do so by using `tmap` for plotting the housing values.

```{r}
mp1 <- tm_shape(msoa.spdf) +
  tm_fill(col = "Value", 
          #style = "cont",
          style = "fisher", n = 8,
          title = "Median", 
          palette = viridis(n = 8, direction = -1, option = "C"),
          legend.hist = TRUE) +
  tm_borders(col = "black", lwd = 1) +
  tm_layout(legend.frame = TRUE, legend.bg.color = TRUE,
            #legend.position = c("right", "bottom"),
            legend.outside = TRUE,
            main.title = "Housing values 2017", 
            main.title.position = "center",
            title.snap.to.legend = TRUE) 

mp1 

```

### Moran's I

Global Moran's I test statistic:
$$		
		\begin{equation} 
		\bm I  = \frac{N}{S_0}	
		\frac{\sum_i\sum_j w_{ij}(y_i-\bar{y})(y_j-\bar{y})}
			{\sum_i (y_i-\bar{y})}, \text{where } S_0 = \sum_{i=1}^N\sum_{j=1}^N w_{ij}
		\end{equation}
$$		

* Relation of the deviation from the mean value between unit $i$ and neighbours of unit $i$. Basically, this measures correlation between neighbouring values.

* Negative values: negative autocorrelation

* Around zero: no autocorrelation

* Positive values: positive autocorrelation

```{r}
# Global Morans I test of housing values based on contiguity weights
moran.test(msoa.spdf$Value, listw = queens.lw, alternative = "two.sided")

# Global Morans I test of housing values based on idw
moran.test(msoa.spdf$Value, listw = idw.lw, alternative = "two.sided")

```

# Spatial Regression Models

There are a bunch of different techniques to model spatial dependence and spatial processes [@LeSage.2009.0]. Here, we will just cover a few of the most common techniques / econometric models. One advantage of the most basic spatial model (SLX) is that this method can easily be incorporated in a variety of other methodologies, such as machine learning approaches. @HalleckVega.2015.0, LeSage.2014.0, and @Ruttenauer.2019c provide article-length introductions.

There are three basic ways of incorporating spatial dependece.

### Spatial Error Model (SEM)

* Clustering on Unobservables

$$
		\begin{equation} 
		\begin{split}
		{\bm y}&=\alpha{\bm \iota}+{\bm X}{\bm \beta}+{\bm u},\\
		{\bm u}&=\lambda{\bm W}{\bm u}+{\bm \varepsilon}
		\end{split} 
		\end{equation}
$$		

### Spatial Autoregressive Model (SAR)

* Interdependence

$$
    \begin{equation} 
		{\bm y}=\alpha{\bm \iota}+\rho{\bm W}{\bm y}+{\bm X}{\bm \beta}+ {\bm \varepsilon}
		\end{equation}  
$$	

### Spatially lagged X Model (SLX)

* Clustering on Spillovers in Covariates

$$
		\begin{equation}
		{\bm y}=\alpha{\bm \iota}+{\bm X}{\bm \beta}+{\bm W}{\bm X}{\bm \theta}+ {\bm \varepsilon}
		\end{equation}
$$	
Moreover, there are models combining two sets of the above specifications.

### Spatial Durbin Model (SDM)

$$
		\begin{equation}
		{\bm y}=\alpha{\bm \iota}+{\bm X}{\bm \beta}+{\bm W}{\bm X}{\bm \theta}+ {\bm \varepsilon}
		\end{equation}
$$	

### Spatial Durbin Error Model (SDEM)

$$
    \begin{equation}
		\begin{split}
		{\bm y}&=\alpha{\bm \iota}+{\bm X}{\bm \beta}+{\bm W}{\bm X}{\bm \theta}+ {\bm u},\\
		{\bm u}&=\lambda{\bm W}{\bm u}+{\bm \varepsilon}
		\end{split}
		\end{equation}
$$

### Combined Spatial Autocorrelation Model (SAC)

$$
		\begin{equation}
		\begin{split}
		{\bm y}&=\alpha{\bm \iota}+\rho{\bm W}{\bm y}+{\bm X}{\bm \beta}+ {\bm u},\\
		{\bm u}&=\lambda{\bm W}{\bm u}+{\bm \varepsilon}
		\end{split}
		\end{equation}
$$

Note that all of these models assume different data generating processes (DGP) leading to the spatial correlation or pattern we observe in the data. Although there are specifications tests, it is generally not possible to let the data decide which one is the true underlying DGP [@Cook.2015.563; @Ruttenauer.2019c]. However, there might be theoretical reasons to guide the model specification [@Cook.2015.563]. 

Just because SAR is probably the model most commonly used does not mean it is the best choice or the most robust alternative. In contrast, various studies [@HalleckVega.2015.0; @Ruttenauer.2019c; @Wimpy.2021] highlight the advantages of the relative simple SLX model. Moreover, this specification can basically be incorporated in any other statistical method.

#### A note on missings

Missing values create a problem in spatial data analysis. For instance, in a local spillover model with an average of 10 neighbours, one initial missing value will lead to 10 missing values in the spatially lagged variable. For global spillover models, one initial missing will 'flow' through the neighbourhood system until the cutoff point (and create a lo of missings). 

Depending on the data, one can either omit units with missings from the data and the initial neighbours weights creation, or we impute the data first, e.g. using interpolation or kriging.

# Spatial Regression Models: Example

To estimate spatial models, we can currently use `spdep` or `spatialreg`. They contain the same functions, but spatial regression models will be made defunct in `spdep` in future releases, and will only be available in `spatialreg`.

### SAR

Lets estimate a spatial SAR model using the `lagsarlm()` with contiguity weights. Here we use the median housing values as depended variable. We include population density (`POPDEN`), the percentage of tree cover (`canopy_per`), and the number of pubs (`pubs_count`) and our traffic estimates (`traffic`)

```{r}
hv_1.sar <- lagsarlm(log(Value) ~ log(POPDEN) + canopy_per + pubs_count + log(traffic),  
                     data = msoa.spdf, 
                     listw = queens.lw,
                     Durbin = FALSE) # we could here extend to SDM
summary(hv_1.sar)

```

This looks pretty much like a conventional model output, with some additional information: a highly significant `hv_1.sar$rho` of `r round(hv_1.sar$rho, 2)` indicates strong positive spatial autocorrelation. In substantive terms, housing prices in the focal unit positively influence housing prices in neighbouring units, which again influences housing prices among the neighbours of these neighbours, and so on.

__NOTE__: interpreting the coefficients or covariates in a SAR model seldomly makes sense, because of the spillovers and feedback loops in $\bm y$ mentioned above!

### SEM

SEM models can be estimated using `errorsarlm()`.

```{r}
hv_1.sem <- errorsarlm(log(Value) ~ log(POPDEN) + canopy_per + pubs_count + traffic,  
                     data = msoa.spdf, 
                     listw = queens.lw,
                     Durbin = FALSE) # we could here extend to SDM
summary(hv_1.sem)

```

In this case `hv_1.sem$lambda` gives us the spatial parameter. A highly significant lambda of `r round(hv_1.sem$lambda)` tells us that the errors are highly spatially correlated (e.g. due to correlated unobservables). In a spatial error model, we can interpret the coefficients directly, as in a conventional linear model. 

### SLX

Above, we could have estimated SDM and SDEM models using the `Durbin` option. To estimate SLX models, we can either use `lmSLX()` directly, or we can create $\bm W \bm X$ manually and use that in any available model fitting function.

```{r}
hv_1.slx <- lmSLX(log(Value) ~ log(POPDEN) + canopy_per + pubs_count + traffic,  
                  data = msoa.spdf, 
                  listw = queens.lw, 
                  Durbin = TRUE) # use a formula to lag only specific covariates
summary(hv_1.slx)

```

In SLX models, we receive the direct effects of covariates as well as the effects of the spatially lagged variables. Here, we can interpret the coefficients directly. 

For instance, lets look at population density: 

1. a high population density in the focal unit is related to lower housing prices, but 

2. a high population density in the neighbouring areas in related to higher housing prices (while keeping population density in the focal unit constant). 

This might indicate that areas with a low population density in central regions of the city (high pop density around) have higher housing prices. We could try testing this interpretation by including the distance to the city centre as a control.

Another way of estimating the same model is lagging the covariates first.

```{r}
# Loop through vars and create lagged variables
msoa.spdf$log_POPDEN <- log(msoa.spdf$POPDEN)
vars <- c("Value", "log_POPDEN", "canopy_per", "pubs_count", "traffic")
for(v in vars){
  msoa.spdf[, paste0("w.", v)] <- lag.listw(queens.lw,
                                            var = st_drop_geometry(msoa.spdf)[, v])
}

# Alternatively:
w_vars <- create_WX(st_drop_geometry(msoa.spdf[, vars]),
                    listw = queens.lw,
                    prefix="w")

head(w_vars)

```

And subsequently we use those new variable in a linear model.

```{r}
hv_1.lm <- lm (log(Value) ~ log(POPDEN) + canopy_per + pubs_count + traffic +
                 w.log_POPDEN + w.canopy_per + w.pubs_count + w.traffic,
               data = msoa.spdf)
summary(hv_1.lm)

```

Looks pretty similar to `lmSLX()` results, and it should! A big advantage of the SLX specification is that we can use the lagged variables in basically all methods which take variables as inputs, such as non-linear models, matching algorithms, and machine learning tools.

Moreover, using the lagged variables gives a high degree of freedom. For instance, we could (not saying that it necessarily makes sense):

* Use different weights matrices for different variables

* Include higher order neighbours using `nblag()` on the neigbours list (with increasing number of orders we go towards a more global model, but we estimate a coefficient for each spillover, instead of estiamting just one)

* Use machine learning techniques to determine the best fitting weights specification


# Impacts

### Coefficient estimates $\neq$ `marginal' effects

__Attention__: Do not interpret coefficients as marginal effects in SAR, SAC, and SDM!! Using the reduced form 

$$	
	\begin{equation}
	\begin{split}
		{\bm y} & =\alpha{\bm \iota}+\rho {\bm W}{\bm y}+{\bm X}{\bm \beta}+{\bm \varepsilon} \\
		{\bm y} & =({\bm I}-\rho{\bm W})^{-1}(\alpha{\bm \iota}+{\bm X}{\bm \beta}+{\bm \varepsilon}),
	\end{split}
	\end{equation}
$$	
	
we can calculate the first derivative:
	
$$	
	\begin{equation}
	\begin{split}
		\frac{\partial \bm y}{\partial \bm x_k} & = ({\bm I}-\rho{\bm W})^{-1}\beta_k \\
		& =({\bm I} + \rho{\bm W} + \rho^2{\bm W}^2 + \rho^3{\bm W}^3 + ...)\beta_k, 
	\end{split}
	\end{equation}	
$$	
	where $\rho{\bm W}\beta_k$ equals the effect stemming from direct neighbours, $\rho^2{\bm W}^2\beta_k$ the effect stemming from second order neighbours (neighbours of neighbours),... This also includes feedback loops if unit $i$ is also a second order neighbour of itself.


# Impacts

Note that the derivatives consist of a matrix, returning individual effects for each unit on each other unit, differentiated in _direct, indirect, and total impacts_. However, the individual effects (how $i$ influences $j$) mainly vary because of variation in ${\bm W}$. Usually, one should use summary measures to report effects in spatial models [@LeSage.2009.0]. @HalleckVega.2015.0 provide a nice summary of the impacts for each model:


Model | Direct Impacts | Indirect Impacts 
:-: | :-: | :-:
OLS/SEM | $\beta_k$ | -- 
SAR/SAC | Diagonal elements of $({\bm I}-\rho{\bm W})^{-1}\beta_k$ | Off-diagonal elements of $({\bm I}-\rho{\bm W})^{-1}\beta_k$ 
SLX/SDEM | $\beta_k$ | $\theta_k$ 
SDM | Diagonal elements of $({\bm I}-\rho{\bm W})^{-1}\left[\beta_k+{\bm W}\theta_k\right]$ | Off-diagonal elements of $({\bm I}-\rho{\bm W})^{-1}$

The different indirect effects / spatial effects mean conceptionally different things:

* Global spillover effects: SAR, SAC, SDM

* Local spillover effects: SLX, SDEM

We can calculate these impacts using `impacts()` with simulated distributions, e.g. for the SAR model:

```{r}
hv_1.sar.imp <- impacts(hv_1.sar, listw = queens.lw, R = 300)
summary(hv_1.sar.imp, zstats = TRUE, short = TRUE)

# Alternative with traces (better for large W)
W <- as(queens.lw, "CsparseMatrix")
trMatc <- trW(W, type="mult")
hv_1.sar.imp2 <- impacts(hv_1.sar, tr = trMatc, R = 300, Q = 10)
summary(hv_1.sar.imp2, zstats = TRUE, short = TRUE)

```

The indirect effects here refer to global spillover effects. This means a change of $x$ in the focal units flows through the entire system of neighbours (direct nieightbours, neighbours of neighbours, ...) influencing $y$. One can think of this as diffusion in time or a change in long-term equilibrium.

For SLX models, nothing is gained from computing the impacts

```{r}
print(impacts(hv_1.slx, listw = queens.lw))

```

# Geographically weighted regression

Obviously there is another interesting questions when using spatial data: Do results vary depending on the region we are looking at. So, we want to exploit the spatial heterogeneity in relation between $y$ and $x$.

GWR [@Brunsdon.1996; @Gollini.2015] is mainly an explorative tool for spatial data analysis in which we estimate an equation at different geographical points. For $L$ given locations across London, we thus receive $L$ different coefficients.

$$
\begin{align} 
\hat{\bm \beta}_l=& ({\bm X}^\intercal{\bm M}_l{\bm X})^{-1}{\bm X}^\intercal{\bm M}_l{\bm Y},
\end{align}
$$

The $N \times N$ matrix ${\bm M}_l$ defines the weights at each local point $l$, assigning higher weights to closer units. For these local weights, we can use various kernel density functions with a predetermined bandwidth $b$ around each point (either a fixed bandwidth or k nearest neighbours). Here, we use `gwr.robust()` of the `GWmodel` package.

```{r}
# Search for the optimal bandwidth 
set.seed(123)
hv_1.bw <- bw.gwr(log(Value) ~ log(POPDEN) + canopy_per + pubs_count + traffic,
                  data = as_Spatial(msoa.spdf),
                  kernel = "boxcar",
                  adaptive = TRUE) 
hv_1.bw


### GWR 
hv_1.gwr <- gwr.robust(log(Value) ~ log(POPDEN) + canopy_per + pubs_count + traffic,
                      data = as_Spatial(msoa.spdf), 
                      kernel = "boxcar", 
                      adaptive = TRUE, 
                      bw = hv_1.bw, 
                      longlat = FALSE)
print(hv_1.gwr)
```

The results give a range of coefficients for different locations. Obviously, we can again map those individual coefficients.

```{r}
# Spatial object
gwr.spdf <- st_as_sf(hv_1.gwr$SDF)
gwr.spdf <- st_make_valid(gwr.spdf)

# Map
tmap_mode("view")

mp2 <- tm_shape(gwr.spdf) +
  tm_fill(col = "canopy_per", 
          style = "hclust", n = 8,
          title = "Coefficient", 
          palette = inferno(n = 8, direction = 1),
          legend.hist = TRUE,
          alpha = 0.6) +
  tm_borders(col = "black", lwd = 1) +
  tm_layout(legend.frame = TRUE, legend.bg.color = TRUE,
            #legend.position = c("right", "bottom"),
            legend.outside = TRUE,
            main.title = "Effect of tree cover", 
            main.title.position = "center",
            title.snap.to.legend = TRUE) 

mp2 

```


# References